{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# carnd_t2_p3_kidnapped_vehicle\n",
    "\n",
    "[//]: # (Image References)\n",
    "[image1]: ./output/failed_d1_R.png\n",
    "[image2]: ./output/failed_d2_R.png\n",
    "[image3]: ./output/failed_d1_RL.png\n",
    "[image4]: ./output/failed_d2_RL.png\n",
    "[image14]: ./output/failed.png\n",
    "[image5]: ./output/passed_d1_R.png\n",
    "[image6]: ./output/passed_d2_R.png\n",
    "[image7]: ./output/passed_d1_RL.png\n",
    "[image8]: ./output/passed_d2_RL.png\n",
    "[image11]: ./output/dataset1_radar_epsilon_distribution.png\n",
    "[image12]: ./output/dataset1_lidar_epsilon_distribution.png\n",
    "\n",
    "\n",
    "## Carnd - term 2 - project 3 - kidnapped vehicle\n",
    "\n",
    "### Overview\n",
    "The goal of this project is to build an Paticle Filter (PF) model to process a series of sensor coordinates (x,y,theta) to exercise the localization of vehicle with respect to global position system(GPS) coordinates. The PF model predicts the vehicle location base on the random generated particles which update their weights by finding their shortest distances from each landmarks in the global map coordinates. Similar to Porject 1 and 2, the PF model is connected to a simulator via uWebSocketIO with predefined landmarks coordinates.\n",
    "\n",
    "\n",
    "### Project Repository\n",
    "All resource are located in Udacity's project repository\n",
    "[CarND-Kidnapped-Vehicle-Project](https://github.com/udacity/CarND-Kidnapped-Vehicle-Project)\n",
    "\n",
    "\n",
    "### Project Submission\n",
    "All modified code including results are committed to my personal github page\n",
    "[carnd_t2_p3_kidnapped_vehicle](https://github.com/chriskcheung/carnd_t2_p3_kidnapped_vehicle)\n",
    "\n",
    "\n",
    "### Key Files\n",
    "##### main.cpp\n",
    "establishes communication between simulator and PF model using uWebSocketIO, and reads in data during set time interval and send sensor measurements to ParticleFilter::init(), ParticleFilter::prediction(), ParticleFilter::updateWeights(), and ParticleFilter::resample() in particle_filter.cpp for processing \n",
    "\n",
    "##### ukf.cpp\n",
    "contains 4 main functions: ParticleFilter::init(), ParticleFilter::prediction(), ParticleFilter::updateWeights(), ParticleFilter::resample().\n",
    "\n",
    "ParticleFilter::init() generates a number (_num_particles_) of particles, then use normal_guassian distribution algorithm to create x,y coordinates offseting from the provided GPS coordinates of the vehicle. Each particles will have a slightly different x,y coordinates to each others. All particles's weight are initialized to 1 at the beginning. \n",
    "\n",
    "ParticleFilter::prediction() is similar to ParticleFilter::init() that it uses normal_guassian distribution algorithm to update all particles's coordinate. Instead of using GPS coordinates of the vehicle, it uses sensor provided data like, vehicle velocity, yaw rate, sensor error deviation, and delta time of each measurements, to calculate the new vehicle location at delta time using trigonometry equations. Once new vehicle location is calculated, normal_guassian distribution is used to introduce new noise to each particles's coordinates as prediction.\n",
    "\n",
    "ParticleFilter::updateWeights() is split into 4 steps: \n",
    "  1) update the list of landmarks each particles should include for calculating particle weights, \n",
    "  2) transform the vehicle observations from vehicle's prespective coordinates to global positioning coordinates,\n",
    "  3) pick the closest landmark to each transformed observation coordinates by finding the shortest distance in between them,\n",
    "  4) apply Multivariate-Gaussian's standard deviation algorithm to transformed observations coordinates and their closest landmark coordinates to calculate their new weights, which is in term the probability of the particle to be picked for determining the vehicle's whereabout in the global position coordinate,\n",
    "  5) finally normalize all particle weights.\n",
    "  \n",
    "ParticleFilter::resample() uses discrete distribution algorithm to pick/sample the particles so that particles with the higher weights/probability will be more likely to be picked/saved as a new set of pacticles for representing the prediction of the vehicle's local position.\n",
    "\n",
    "\n",
    "##### side note\n",
    "Due to the predefined input parameter of dataAssociation() is not flexible to use, I decided to handle data association in updateWeight() instead of using dataAssociation() in my implemenation. \n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "## Implementation Challenge\n",
    "\n",
    "### Initialization\n",
    "Applies normal distribution algorithm directly to GPS x,y coordinates and angle theta in radian directly and use for loop to create new particles with different offset within the standard deviation of which x,y,theta.  \n",
    "\n",
    "```c++\n",
    "    void ParticleFilter::init(double x, double y, double theta, double std[]) {\n",
    "        ...\n",
    "        default_random_engine gen;\n",
    "        normal_distribution<double> dist_x(x, std[0]);\n",
    "        normal_distribution<double> dist_y(y, std[1]);\n",
    "        normal_distribution<double> dist_theta(theta, std[2]);\n",
    "        ...\n",
    "```\t\n",
    "\n",
    "\n",
    "### Prediction\n",
    "First, new x,y,theta are calculating from the vehicle velocity and yawrate.\n",
    "\n",
    "```c++\n",
    "\t\tif (yaw_rate != 0.0f){\n",
    "\t\t\tx = x + velocity/yaw_rate*( sin(theta + yaw_rate*delta_t) - sin(theta));\n",
    "\t\t\ty = y + velocity/yaw_rate*(-cos(theta + yaw_rate*delta_t) + cos(theta));\n",
    "\t\t\ttheta = theta + yaw_rate*delta_t;\n",
    "\t\t}\n",
    "\t\telse {\n",
    "\t\t\tx = x + velocity*cos(theta)*delta_t;\n",
    "\t\t\ty = y + velocity*sin(theta)*delta_t;\n",
    "\t\t\ttheta = theta;\n",
    "\t\t}\n",
    "```\n",
    "\n",
    "Prediction is a bit different than initialization which it comes to using normal distribution algorithm. The algorithm is used to generate noise to the newly predicted x,y, and theta. \n",
    "\n",
    "```c++\n",
    "\tdefault_random_engine gen;\n",
    "\tnormal_distribution<double> dist_x(0, std_pos[0]);\n",
    "\tnormal_distribution<double> dist_y(0, std_pos[1]);\n",
    "\tnormal_distribution<double> dist_theta(0, std_pos[2]);\n",
    "    ...\n",
    "\t\tparticles[i].x = x +dist_x(gen);\n",
    "\t\tparticles[i].y = y+dist_y(gen);\n",
    "\t\tparticles[i].theta = theta+dist_theta(gen);\n",
    "```\n",
    "\n",
    "\n",
    "### Update Weight\n",
    "Since the sensor range is provided, there is no point to process landmarks that are outside or the sensor range. Therefore, reducing the landmarks size is recommended and it helps to improve the filter performance.\n",
    "\n",
    "```c++\n",
    "    for (int j=0; j<map_landmarks.landmark_list.size(); j++){\n",
    "        if (dist(particles[i].x, particles[i].y, map_landmarks.landmark_list[j].x_f, map_landmarks.landmark_list[j].y_f) <= sensor_range){\n",
    "            LandmarkObs lmObs;\n",
    "            lmObs.id = map_landmarks.landmark_list[j].id_i;\n",
    "            lmObs.x  = map_landmarks.landmark_list[j].x_f;\n",
    "            lmObs.y  = map_landmarks.landmark_list[j].y_f;\n",
    "            reduced_map.push_back(lmObs);\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "Next, transform vehicle's obsersation from vehicle's coordinates to global positioning coordinates so all datas are in the same coordination system.\n",
    "\n",
    "```c++\n",
    "    for (int j=0; j<observations.size(); j++){\n",
    "        LandmarkObs tmp;\n",
    "        tmp.x = particles[i].x + observations[j].x*cos(particles[i].theta) - observations[j].y*sin(particles[i].theta);\n",
    "        tmp.y = particles[i].y + observations[j].y*cos(particles[i].theta) + observations[j].x*sin(particles[i].theta);\n",
    "        transformed_obs.push_back(tmp);\n",
    "        ...\n",
    "```\n",
    "\n",
    "To associate each transformed observations to their closest landmarks, find their shortest distance. To simplify the code, I used the id field in transformed observations to hold the corresponding landmark's id.\n",
    "\n",
    "```c++\n",
    "        for (int k = 0; k < reduced_map.size(); k++) {\n",
    "            double diff = dist(tmp.x, tmp.y, reduced_map[k].x, reduced_map[k].y);\n",
    "            if (diff < shortest) {\n",
    "                shortest = diff;\n",
    "                transformed_obs[j].id = reduced_map[k].id;\n",
    "                shortest_idx = k;\n",
    "            }\n",
    "        }\n",
    "```\n",
    "\n",
    "\n",
    "To further improve performance, drop the landmark from the reduced_map list once it is associated to a transformed observation assuming a one-to-one relationship between each observation and its corresponding landmark, and no more than one landmark will be paired with each observation. If an landmark is paired to a wrong observation, their big distance will result to smaller weight and lowering its probabity to be picked later on during resampling. \n",
    "\n",
    "```c++\n",
    "        // optimize association finding by illiminating the landmark that are just associated to a transformed observation\n",
    "        // about 15% improvement on timing\n",
    "        if (shortest < std::numeric_limits<double>::infinity()){\n",
    "            reduced_map.erase(reduced_map.begin() + shortest_idx);\n",
    "        }\n",
    "```\n",
    "\n",
    "Use Multivariate-Gaussian's standard deviation algorithm to find the sum of product between particles and associated as weight update.\n",
    "\n",
    "```c++\n",
    "\t\tfor (int j=0; j<transformed_obs.size(); j++){\n",
    "\t\t\tdouble x = transformed_obs[j].x;\n",
    "\t\t\tdouble y = transformed_obs[j].y;\n",
    "\t\t\t//double ux = map_landmarks.landmark_list[transformed_obs[j].id - 1].x_f;\n",
    "\t\t\t//double uy = map_landmarks.landmark_list[transformed_obs[j].id - 1].y_f;\n",
    "\t\t\tdouble ux = map_landmarks.landmark_list[transformed_obs[j].id - 1].x_f;\n",
    "\t\t\tdouble uy = map_landmarks.landmark_list[transformed_obs[j].id - 1].y_f;\n",
    "\t\t\tdouble diffx2 = (x-ux) * (x - ux);\n",
    "\t\t\tdouble diffy2 = (y-uy) * (y - uy);\n",
    "\t\t\tdouble sqrt2pr= (2.0f*M_PI*std_landmark[0]*std_landmark[1]);\n",
    "\t\t\tnew_weight *= exp(-(diffx2/(2.0f*stdx2)) - (diffy2/(2.0f*stdy2)))/sqrt2pr;\n",
    "\t\t}\n",
    "\t\tweights[i] = new_weight;\n",
    "\t\tparticles[i].weight = new_weight;\n",
    "```\n",
    "\n",
    "\n",
    "Lastly, normalized weights.\n",
    "\n",
    "```c++\n",
    "    // normalized all weights by dividing each weight element with sum of all weights\n",
    "\tdouble norm_weight = std::accumulate(weights.begin(), weights.end(), 0.0f);\n",
    "\tif (norm_weight > 0) {\n",
    "\t\tfor (int i =0; i<num_particles; i++){\n",
    "\t\t\tweights[i] /= norm_weight*1.0f;\n",
    "\t\t\tparticles[i].weight /= norm_weight;\n",
    "\t\t}\n",
    "\t}\n",
    "```\n",
    "\n",
    "### Resample\n",
    "\n",
    "I originally use Sabastian's Resampling Wheel algorithm, it works but it is slower than using discrete distribution algorithm, which discrete distribution offers 15% faster. As a result, discrete distribution is used.\n",
    "\n",
    "```c++\n",
    "\tdefault_random_engine gen;\n",
    "\tdiscrete_distribution<> ddist(weights.begin(), weights.end());\n",
    "\t\n",
    "\tstd::vector<Particle> new_particles;\n",
    "\tfor (int i = 0; i < num_particles; i++) {\n",
    "\t\tnew_particles.push_back(particles[ddist(gen)]);\n",
    "\t}\n",
    "\tparticles = new_particles;\n",
    "```\n",
    "\n",
    "\n",
    "## Result\n",
    "In the first running version, the particle was following the vehicle only at the first few time step and then it drifted away upward from the vehicle. It still followed the vehicle direction as it traveled, but eventually run outside of the max error requirement. It was due to a few typo to the Multivariate-Gaussian's standard deviation algorithm and observation transformation. After fixing the bugs, the particles were following the vehicle closely as it travel. I started using 100 particles, which was extremely slow. I dropped it to 15, and I saw huge improvement. I further reduced it to 8, 7, then 6 and started to notice the paticle would drift away at 6 particles. So I finalized to 7 particles as the most optimal size to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
